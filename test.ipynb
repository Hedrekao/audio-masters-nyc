{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def extract_first_channel(audio_path, output_path):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path, sr=None, mono=False)\n",
    "    \n",
    "    # Extract the first channel\n",
    "    print(len(y))\n",
    "    channel_1 = y[0]\n",
    "    \n",
    "    # Save the first channel as a mono audio file\n",
    "    sf.write(output_path, channel_1, sr)\n",
    "\n",
    "# Path to your 32-channel audio file\n",
    "input_audio_path = 'audio.wav'\n",
    "\n",
    "# Output path for the first channel\n",
    "output_audio_path = 'single.wav'\n",
    "\n",
    "# Call the function to extract the first channel\n",
    "extract_first_channel(input_audio_path, output_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_channel, sr = librosa.load(output_audio_path, sr=None)\n",
    "\n",
    "print(len(single_channel))\n",
    "\n",
    "#plot amplitude to time graph\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(np.arange(len(single_channel)) / sr, single_channel)\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spectrogram\n",
    "\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(single_channel)), ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "plt.title('Spectrogram')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot amplitude to frequency graph (x-axis is frequency)\n",
    "\n",
    "# perform Fourier transform\n",
    "fft = np.fft.fft(single_channel)\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrum = np.abs(fft)\n",
    "# create frequency variable\n",
    "f = np.linspace(0, sr, len(spectrum))\n",
    "# take half of the spectrum and frequency\n",
    "left_spectrum = spectrum[:int(len(spectrum)/2)]\n",
    "left_f = f[:int(len(spectrum)/2)]\n",
    "# plot spectrum\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(left_f, left_spectrum, alpha=0.4)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.title(\"Power spectrum\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot spectiogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_audio, sr = librosa.load('audio.wav', sr=None, mono=None)\n",
    "\n",
    "print(f\"number of channels: {full_audio.shape[0]}\")\n",
    "\n",
    "print(f\"len in seconds: {full_audio.shape[1] / sr}\")\n",
    "\n",
    "# print each channel waveform on subgrid one below the other\n",
    "\n",
    "plt.figure(figsize=(15,25))\n",
    "\n",
    "for i in range(full_audio.shape[0]):\n",
    "    plt.subplot(full_audio.shape[0], 1, i+1)\n",
    "    plt.plot(full_audio[i])\n",
    "    plt.title(f\"Channel {i+1}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which channel hears the sound first\n",
    "# Calculate the onset strength for each channel\n",
    "onset_strengths = [librosa.onset.onset_strength(y=channel, sr=sr) for channel in full_audio]\n",
    "\n",
    "\n",
    "onset_times = [librosa.onset.onset_detect(onset_envelope=onset_strength, sr=sr)[0] for onset_strength in onset_strengths]\n",
    "\n",
    "closest_mic_channel = np.argmin(onset_times) + 1\n",
    "furthest_mic_channel = np.argmax(onset_times) + 1\n",
    "\n",
    "print(onset_times)\n",
    "\n",
    "print(closest_mic_channel, furthest_mic_channel)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_audio = full_audio[:, :3*sr]\n",
    "\n",
    "clean_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_am = np.mean(np.abs( clean_audio ), axis=1)\n",
    "\n",
    "mean_am.shape\n",
    "# scale mean amplitude to be between 0 and 1\n",
    "\n",
    "\n",
    "print(mean_am)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "print(mean_am)\n",
    "\n",
    "plt.bar(np.arange(mean_am.shape[0]), mean_am)\n",
    "plt.xticks(np.arange(mean_am.shape[0]), np.arange(1, mean_am.shape[0]+1))\n",
    "plt.xlabel(\"Channel\")\n",
    "plt.ylabel(\"Mean amplitude\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# cmaybe channel 1,3 are closes to speaker because they have the highest mean amplitude during the fragment\n",
    "# when the speaker is talking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "## use fast ica to decompose single track into 2 components and save the components as new audio files\n",
    "four_channels = full_audio[:8]\n",
    "print(four_channels.shape)\n",
    "\n",
    "ica = FastICA(n_components=8, whiten='arbitrary-variance')\n",
    "\n",
    "\n",
    "components = ica.fit_transform(four_channels.T)\n",
    "\n",
    "print(components.shape)\n",
    "\n",
    "# make the components louder\n",
    "\n",
    "components = components * 100\n",
    "\n",
    "# save the components as audio files\n",
    "\n",
    "for i in range(components.shape[1]):\n",
    "\n",
    "    sf.write(f'component_{i+1}.wav', components[:, i], sr)\n",
    "\n",
    "\n",
    "# now do pca\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=8)\n",
    "\n",
    "components = pca.fit_transform(four_channels.T)\n",
    "\n",
    "print(components.shape)\n",
    "\n",
    "# make the components louder\n",
    "\n",
    "components = components * 100\n",
    "\n",
    "\n",
    "# save the components as audio files\n",
    "\n",
    "for i in range(components.shape[1]):\n",
    "\n",
    "    sf.write(f'pca_component_{i+1}.wav', components[:, i], sr)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do ICA on frequency domain\n",
    "four_channels = full_audio[:]\n",
    "\n",
    "four_channels =  four_channels[:]\n",
    "\n",
    "from ssspy.bss.ica import FastICA\n",
    "\n",
    "\n",
    "def contrast_fn(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def score_fn(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_score_fn(x):\n",
    "    sigma = 1 / (1 + np.exp(-x))\n",
    "    return sigma * (1 - sigma)\n",
    "     \n",
    "\n",
    "ica = FastICA(\n",
    "    contrast_fn=contrast_fn,\n",
    "    score_fn=score_fn,\n",
    "    d_score_fn=d_score_fn,\n",
    ")\n",
    "print(ica)\n",
    "     \n",
    "import IPython.display as ipd\n",
    "\n",
    "waveform_est = ica(four_channels, n_iter=10)\n",
    "\n",
    "for idx, waveform in enumerate(waveform_est):\n",
    "    print(\"Estimated source: {}\".format(idx + 1))\n",
    "    # TODO uncomment this line to dispaly new audios, I had to comment it out because of the size\n",
    "    # display(ipd.Audio(waveform, rate=sr))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
